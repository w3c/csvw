<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta content="width=device-width,initial-scale=1" name="viewport" />
    <title>CSV on the Web: Use Cases and Requirements</title>
    <script class="remove" src="http://www.w3.org/Tools/respec/respec-w3c-common">
		</script>
    <script class="remove">
var respecConfig = {
    specStatus: "ED",
    shortName: "csvw-use-cases-and-reqs",
    //publishDate:  "2012-12-12",
    //previousPublishDate:  "",
    edDraftURI: "http://w3c.github.io/csvw/use-cases-and-requirements/",
    // lcEnd: "3000-01-01",
    // crEnd: "3000-01-01",
    editors: [{
            name: "Jeremy Tandy",
            company: "Met Office",
            companyURL: "http://www.metoffice.gov.uk/"
        },{
	  name: "Davide Ceolin",
	  company: "VU University Amsterdam",
	  companyURL: "http://www.vu.nl"
	}],
    wg: "CSV on the Web",
    wgURI: "https://www.w3.org/2013/csvw/wiki/Main_Page",
    wgPublicList: "public-csv-wg",
    wgPatentURI: "http://www.w3.org/2001/tag/disclosures",
    otherLinks: [{
            key: "Repository",
            data: [{
                    value: "We are on Github.",
                    href: "https://github.com/w3c/csvw"
                }, {
                    value: "File a bug.",
                    href: "https://github.com/w3c/csvw"
                }, {
                    value: "Commit history.",
                    href: "https://github.com/w3c/csvw/commits/gh-pages"
                }
            ]
        }
    ],
    inlineCSS: true,
    noIDLIn: true,
    noLegacyStyle: false
    };
    </script>
    <link href="editorial.css" rel="stylesheet" type="text/css" />
  </head>
  <body>
    <section id="abstract">
      <p> A large percentage of the data published on the Web is tabular data, commonly published as
        comma separated values (CSV) files. The CSV on the Web Working Group aim to specify
        technologies that provide greater interoperability for data dependent applications on the
        Web when working with tabular datasets comprising single or multiple files using CSV, or
        similar, format. </p>
      <p> This document lists the use cases compiled by the Working Group that are considered
        representative of how tabular data is commonly used within data dependent applications. The
        use cases observe existing common practice undertaken when working with tabular data, often
        illustrating shortcomings or limitations of existing formats or technologies. This document
        also provides a set of requirements derived from these use cases that have been used to
        guide the specification design. </p>
    </section>
    <section id="sotd">
      <p> This is a draft document which may be merged into another document or eventually make its
        way into being a standalone Working Draft. </p>
    </section>
    <section id="intro">
      <h2>Introduction</h2>
      <p> A large percentage of the data published on the Web is tabular data, commonly published as
        comma separated values (CSV) files. CSV files may be of a significant size but they can be
        generated and manipulated easily, and there is a significant body of software available to
        handle them. Indeed, popular spreadsheet applications (Microsoft Excel, iWork’s Number, or
        OpenOffice.org) as well as numerous other applications can produce and consume these files.
        However, although these tools make conversion to CSV easy, it is resisted by some publishers
        because CSV is a much less rich format that can't express important detail that the
        publishers want to express, such as annotations, the meaning of identifier codes etc. </p>
      <p> Existing formats for tabular data are format-oriented and hard to process (e.g. Excel);
        un-extensible (e.g. CSV/TSV); or they assume the use of particular technologies (e.g. SQL
        dumps). None of these formats allow developers to pull in multiple data sets, manipulate,
        visualize and combine them in flexible ways. Other information relevant to these datasets,
        such as access rights and provenance, is not easy to find. CSV is a very useful and simple
        format, but to unlock the data and make it portable to environments other than the one in
        which it was created, there needs to be a means of encoding and associating relevant
        metadata. </p>
      <p> To address these issues, the CSV on the Web Working Group seeks to provide: </p>
      <ul>
        <li>Metadata vocabulary for CSV data</li>
        <li>Access methods for CSV Metadata</li>
        <li>Mapping mechanism to transforming CSV into various Formats (e.g., RDF, JSON, or
          XML)</li>
      </ul>
      <p> In order to determine the scope of and elicit the requirements for this <i>extended</i>
        CSV format (CSV+) a set of use cases have been compiled. Each use case provides a narrative
        describing how a representative user works with tabular data to achieve their goal,
        supported, where possible, with example datasets. The use cases observe existing common
        practice undertaken when working with tabular data, often illustrating shortcomings or
        limitations of existing formats or technologies. It is anticipated that the additional
        metadata provided within the CSV+ format, when coupled with metadata-aware tools, will
        simplify how users work with tabular data. As a result, the use cases seek to identify where
        user effort may be reduced. </p>
      <p> A set of requirements, used to guide the development of the CSV+ specification, have been
        derived from the compiled use cases. </p>
    </section>
    <section id="term">
      <h2>Terminology</h2>
      <p> ... </p>
    </section>
    <section id="uc">
      <h2>Use Cases</h2>
      <section id="UC-DigitalPreservationOfGovernmentRecords">
        <h2>Use Case #1 - Digital preservation of government records</h2>
        <p>
          <span style="font-size: 10pt">(Contributed by Adam Retter)</span>
        </p>
        <p>The laws of England and Wales place obligations upon departments and <a
            href="http://www.nationalarchives.gov.uk/">The National Archives</a> for the collection,
          disposal and preservation of records. Government departments are obliged within the <a
            href="http://www.nationalarchives.gov.uk/information-management/legislation/public-records-act.htm"
            >Public Records Act 1958</a> sections 3, 4 and 5 to select, transfer, preserve and make
          available those records that have been defined as public records. These obligations apply
          to records in all formats and media, including paper and digital records. Details
          concerning the selection and transfer of records can be found <a
            href="http://www.nationalarchives.gov.uk/information-management/our-services/selection-and-transfer.htm"
            >here</a>.</p>
        <p>Departments transferring records to TNA must catalogue or list the selected records
          according to The National Archives' defined cataloguing principles and standards.
          Cataloguing is the process of writing a description, or <em>Transcriptions of Records</em>
          for the records being transferred. Once each Transcription of Records is added to the
          Records Catalogue, records can be subsequently discovered and accessed using the supplied
          descriptions and titles.</p>
        <p>TNA specifies what information should be provided within a Transcriptions of Records and
          how that information should be formatted. A number of formats and syntaxes are supported,
          including RDF. However, the predominant format used for the exchange of Transcriptions of
          Records is CSV as the government departments providing the Records lack either the
          technology or resources to provide metadata in the XML and RDF formats preferred by the
          TNA.</p>
        <p>A CSV-encoded Transcriptions of Records typically describes a set of Records, often
          organised within a hierarchy. As a result, it is necessary to describe the
          interrelationships between Records within a single CSV file.</p>
        <p>Each row within a CSV file relates to a particular Record and is allocated a unique
          identifier. This unique identifier behaves as a primary key for the Record within the
          scope of the CSV file and is used when referencing that Record from within other Record
          transcriptions.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-PrimaryKey">PrimaryKey</a> and <a href="#R-ForeignKeyReferences"
            >ForeignKeyReferences</a>
        </p>

        <p>Upon receipt by TNA, each of the Transcriptions of Records is validated against the (set
          of) centrally published data definition(s); it is essential that received CSV metadata
          comply with these specifications to ensure efficient and error free ingest into the
          Records Catalogue.</p>
        <p>The validation applied is dependent the type of entity described in each row. Entity type
          is specified in a specific column (e.g. <code>type</code>).</p>
        <p>The data definition file, or CSV Schema, used by the CSV Validation Tool effectively
          forms the basis of a formal contract between TNA and supplying organisations. For more
          information on the CSV Validation Tool and CSV Schema developed by TNA please refer to the
            <a href="https://github.com/digital-preservation/csv-validator#csv-validator">online
            documentation</a>. </p>
        <p>The CSV Validation Tool is written in <a href="http://www.scala-lang.org/">Scala</a>
          version 2.10.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-ExternalDataDefinitionResource">ExternalDataDefinitionResource</a> and <a
            href="#R-CsvValidation">CsvValidation</a>
        </p>

        <p>Following validation, the CSV-encoded Transcriptions of Records are transformed into RDF
          for insertion into the triple store that underpins the Records Catalogue. The CSV files do
          not include all the information required to undertake the transformation, e.g. defining
          which RDF properties are to be used when creating triples for the data value in each cell.
          As a result, bespoke software has been created by TNA to supply the necessary additional
          information during the CSV to RDF transformation process. The availability of generic
          mechanisms to transform CSV to RDF would reduce the burden of effort within TNA when
          working with CSV files.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-SyntacticTypeDefinition">SyntacticTypeDefinition</a>, <a
            href="#R-SemanticTypeDefinition">SemanticTypeDefinition</a> and <a
            href="#R-CsvToRdfTransformation">CsvToRdfTransformation</a>
        </p>

        <div class="issue">
          <p>Reference to Data Definition Resource (<a>DDR</a>) for a transcription of records is
            required.</p>
          <p>Provide reference to TNA Records <a href="http://discovery.nationalarchives.gov.uk/"
              >discovery catalogue</a>.</p>
          <p>Use case also requires a real example to refer to; both the CSV metadata and a
            reference into the catalogue. Please can this show how the row type leads to different
            validation being applied?</p>
          <p>Resulting RDF transformed from the example CSV is required.</p>
          <p>Please indicate what software language and/or tools/libraries are used to work with the
            CSV data - both validation and <em>transformation</em>.</p>
        </div>

      </section>
      <section id="UC-PublicationOfNationalStatistics">
        <h2>Use Case #2 - Publication of National Statistics</h2>
        <p>
          <span style="font-size: 10pt">(Contributed by Jeni Tennison)</span>
        </p>
        <p>The <a href="http://www.ons.gov.uk/">Office for National Statistics</a> (ONS) is the UK’s
          largest independent producer of official statistics and is the recognised national
          statistical institute for the UK. It is responsible for collecting and publishing
          statistics related to the economy, population and society at national, regional and local
          levels.</p>
        <p>Sets of statistics are typically grouped together into collections of tabular data for
          publication. Example collections of statistics include:</p>
        <ul>
          <li>
            <a
              href="http://www.ons.gov.uk/ons/rel/ppi2/producer-prices-indices/december-2013/rft-mm22-producer-price-indices--reference-table--december-2013.xls"
              >producer prices indices (Microsoft Excel Workbook)</a>
          </li>
          <li>
            <a
              href="http://www.ons.gov.uk/ons/rel/cpi/consumer-price-indices/december-2013/consumer-price-inflation-reference-tables.xls"
              >consumer prices indices (Microsoft Excel Workbook)</a>
          </li>
          <li>
            <a
              href="http://www.ons.gov.uk/ons/rel/lms/labour-market-statistics/january-2014/table-a01.xls"
              >labour market statistics (Microsoft Excel Workbook)</a>
          </li>
        </ul>
        <p>In each of the examples above, multiple data tables are provided in a single Microsoft
          Excel Workbook, one table per sheet. Each Workbook includes a manifest of the tables
          included therein - provided as a table of contents. Furthermore, each Workbook also
          provides metadata pertaining to the whole package. For example, the <a
            href="http://www.ons.gov.uk/ons/rel/ppi2/producer-prices-indices/december-2013/rft-mm22-producer-price-indices--reference-table--december-2013.xls"
            >producer prices indices</a> workbook includes a coversheet that specifies: </p>
        <ul>
          <li>title</li>
          <li>applicability interval (e.g. "Data for December 2013")</li>
          <li>editor and publication office</li>
          <li>publication date, and</li>
          <li>license</li>
        </ul>
        <p>Several tables may appear within a single sheet; for example, refer to <a
            href="http://www.ons.gov.uk/ons/rel/lms/labour-market-statistics/january-2014/table-a01.xls"
            >labour market statistics</a> sheet 18(1). Here we see statistics relating to headline
          estimates, change on quarter and change on year. A closer inspection indicates that the
          column layout is identical for each sub-table; only the meaning of the data values
          changes.</p>
        <div class="issue">
          <p>Do we want to be able to assert within the CSV+ metadata that the "data" exists at a
            particular region within the CSV? When talking about multiple tables within a single CSV
            file, AndyS <a
              href="http://lists.w3.org/Archives/Public/public-csv-wg/2014Feb/0108.html"
            >stated</a>:</p>
          <p> "<em>Maybe put the location of the data table within a single CSV file into the
              associated metadata: a package description for a single file.</em>" </p>
        </div>
        <div class="issue">
          <p>"FWIW, I would only take syntactic requirements from published “CSVs”, not from
            non-text-based formats like Excel. So, for example, I wouldn’t use the ONS Excel files
            as demonstrating a requirement to have multiple tables within a single CSV file."</p>
          <p>
            <span style="font-size: 10pt">[Jeni Tennison]</span>
          </p>
        </div>

        <p>
          <strong>Requires:</strong>
          <a href="#R-PackagingOfMultipleTables">PackagingOfMultipleTables</a>
        </p>
        <p>Correct interpretation of the statistics often requires additional qualification or
          awareness of context. To achieve this ONS routinely include supplementary information and
          annotation within their published statistics. These may be applied to:</p>
        <ul>
          <li>a package of tables; e.g. the entire Workbook</li>
          <li>an individual sheet within the Workbook</li>
          <li>a range of cells within a sheet</li>
          <li>a row</li>
          <li>a coloumn</li>
          <li>an individual cell</li>
        </ul>
        <p>The layout of tabular data within each sheet is optimised for human consumption.
          Presentation artifacts are typically intermingled with the data itself making it difficult
          for data reusers to extract data from these tables for further processing. Presentation
          artifacts, some of which are illustrated in figure <a
            href="#fig-presentation-artifacts-in-statistics-worksheet"></a>, include:</p>
        <ul>
          <li>titles for tables and sub-tables</li>
          <li>annotations</li>
          <li>graphical logos and other formatting</li>
          <li>column ordering - whereby proximity implies some implicit relationship</li>
          <li>merged cells within column headers to indicate grouping</li>
          <li>column headers spanning multiple rows</li>
        </ul>
        <figure>
          <img src="a01jan2014_tcm77-347639_sheet-18(1).PNG" width="909" height="454" />
          <figcaption>Presentation artifacts in statistics worksheet</figcaption>
        </figure>
        <div class="issue">
          <p>Proposal (above) to focus only on text-based CSVs, not "rich" formats like Excel.
            Should this discussion on "human-oriented formatting" be removed from this use case?
            (Although I note that similar issues happen elsewhere in text-based "CSV" where data has
            been normalised to suit human readers; e.g. where monthly temperature values for a given
            year are all given in a single row.)</p>
        </div>
        <p>
          <strong>Requires:</strong>
          <a href="#R-AnnotationAndSupplementaryInfo">AnnotationAndSupplementaryInfo</a>
        </p>
        <p>Furthermore, these statistical data sets make frequent use of predefined category codes
          and geographic regions. At present there is no mechanism to associate the catagory codes,
          provided as plain text, with their authoritative definitions.</p>
        <p>
          <strong>Requires:</strong>
          <a href="#R-AssociationOfCodeValuesWithExternalDefinitions"
            >AssociationOfCodeValuesWithExternalDefinitions</a>
        </p>
        <p>Finally, reuse of the statistical data is also inhibited by a lack of explicit definition
          of the meaning of column headings.</p>
        <p>
          <strong>Requires:</strong>
          <a href="#R-SemanticTypeDefinition">SemanticTypeDefinition</a>
        </p>
        <p>Statistical data is currently published in Microsoft Excel Workbook format in order
          overcome limitations inherent in simple data formats such as CSV. However, it is also
          important to remember that the data user community also have preferred tools for consuming
          the statistical data. Spreadsheet applications such as Microsoft Excel, Numbers for Mac
          and LibreOffice Calc are a mainstay of the ubiquitously deployed desktop productivity
          software, enabling a large swath of users to work with data provided in Microsoft Excel
          Workbook format. It is important that compatibility, at least at a basic level, is
          maintained with the data analysis tools in common usage. </p>
        <p>
          <strong>Requires:</strong>
          <a href="#R-ZeroEditCompatibility">ZeroEditCompatibility</a>
        </p>
      </section>
      <section id="UC-SurfaceTemperatureDatabank">
        <h2>Use Case #3 - Creation of consolidated global land surface temperature climate
          databank</h2>
        <p>
          <span style="font-size: 10pt">(Contributed by Jeremy Tandy)</span>
        </p>
        <p> Climate change and global warming have become one of the most pressing environmental
          concerns in society today. Crucial to predicting future change is an understanding of how
          the world’s historical climate, with long duration instrumental records of climate being
          central to that goal. Whilst there is an abundance of data recording the climate at
          locations the world over, the scrutiny under which climate science is put means that much
          of this data remains unused leading to a paucity of data in some regions with which to
          verify our understanding of climate change.</p>

        <p>The <a href="http://www.surfacetemperatures.org/home">International Surface Temperature
            Initiative</a> seeks to create a consolidated global land surface temperatures databank
          as an open and freely available resource to climate scientists.</p>

        <p>To achieve this goal, climate datasets, known as “decks”, are gathered from participating
          organisations and merged into a combined dataset using a scientifically peer reviewed <a
            href="ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage3/merging_methodology.pdf"
            > method</a> which assesses the data records for inclusion against a variety of
          criteria.</p>

        <p>Given the need for openness and transparency in creating the databank, it is essential
          that the provenance of the source data is clear. Original source data, particularly for
          records captured prior to the mid-twentieth century, may be in hard-copy form. In order to
          incorporate the widest possible scope of source data, the International Surface
          Temperature Initiative is supported by <a
            href="http://www.surfacetemperatures.org/databank/data-rescue-task-team">data rescue
            activities</a> to digitise hard copy records.</p>

        <p>The data is, where possible, published in the following four stages:</p>
        <ul>
          <li>Stage 0: raw digital image of hard copy records or information as to hard copy
            location</li>
          <li>Stage 1: data in native format provided</li>
          <li>Stage 2: data converted into a common format and with provenance and version control
            information appended</li>
          <li>Stage 3: merged collation of stage 2 data within a single consolidated dataset</li>
        </ul>

        <p>The Stage 1 data is typically provided in tabular form - the most common variant is
          white-space delimited ASCII files. Each data deck comprises multiple files which are
          packaged as a compressed tar ball (<code>.tar.gz</code>). Included within the compressed
          tar ball package, and provided alongside, is a read-me file providing unstructured
          supplementary information. Summary information is often embedded at the top of each
          file.</p>

        <p>For example, see the <a
            href="ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage1/uganda/uganda.monthly.stage1.20120301.tar.gz"
            > Ugandan Stage 1 data deck</a> and associated <a
            href="ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage1/uganda/1-readme-uganda.txt"
            > readme file</a>.</p>

        <p>The Ugandan Stage 1 data deck appears to be comprised of two discrete datasets, each
          partitioned into a sub-directory within the tar ball: <code>uganda-raw</code> and
            <code>uganda-bestguess</code>. Each sub-directory includes a Microsoft Word document
          providing supplementary information about the provenance of the dataset; of particular
          note is that <code>uganda-raw</code> is collated from 9 source datasets whilst
            <code>uganda-bestguess</code> provides what is considered by the data publisher to be
          the best set of values with duplicate values discarded.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-AnnotationAndSupplementaryInfo">AnnotationAndSupplementaryInfo</a>
        </p>

        <p>Dataset <code>uganda-raw</code> is split into 96 discrete files, each providing maximum,
          minimum or mean monthly air temperature for one of the 32 weather observation stations
          (sites) included in the data set. Similarly, dataset <code>uganda-bestguess</code> is
          partitioned into discrete files; this case just 3 files each of which provide maximum,
          minimum or mean monthly air temperature data for all sites. The mapping from data file to
          data sub-set is described in the Microsoft Word document.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-CsvAsSubsetOfLargerDataset">CsvAsSubsetOfLargerDataset</a>
        </p>

        <p>A snippet of the data indicating maximum monthly temperature for Entebbe, Uganda, from
            <code>uganda-raw</code> is provided below. File = <a type="text/plain"
            href="637050_ENTEBBE_tmx.txt">637050_ENTEBBE_tmx.txt</a></p>

        <pre class="example">
            637050  ENTEBBE
            5
            ENTEBBE BEA     0.05    32.45   3761F
            ENTEBBE GHCNv3G 0.05    32.45   1155M
            ENTEBBE ColArchive      0.05    32.45   1155M
            ENTEBBE GSOD    0.05    32.45   1155M
            ENTEBBE NCARds512       0.05    32.755  1155M
            
            Tmax
            {snip}
            1935.04	27.83	27.80	27.80	-999.00	-999.00
            1935.12	25.72	25.70	25.70	-999.00	-999.00
            1935.21	26.44	26.40	26.40	-999.00	-999.00
            1935.29	25.72	25.70	25.70	-999.00	-999.00
            1935.37	24.61	24.60	24.60	-999.00	-999.00
            1935.46	24.33	24.30	24.30	-999.00	-999.00
            1935.54	24.89	24.90	24.90	-999.00	-999.00
            {snip}
          </pre>

        <p>The key characteristics are:</p>
        <ul>
          <li>white space delimited; this is not strictly a CSV file</li>
          <li>summary information pertinent to the “data rows” is included at the beginning of the
            data file</li>
          <li>row, column and cell value interpretation is informed by accompanying Microsoft Word
            document; human intervention is required to unambiguously determine semantics, e.g. the
            meaning of each column, the unit of measurement</li>
          <li>the observed property is defined as “Tmax”; there is no reference to an authoritative
            definition describing that property</li>
          <li>there is no header line providing column names </li>
          <li>the year and month (column 1) is expressed as a decimal value; e.g. 1901.04 –
            equivalent to January, 1901</li>
          <li>multiple temperature values (“replicates”) are provided for each row; one from each of
            the sources defined in the header, e.g. <code>BEA</code> (British East Africa),
              <code>GHCNv3G</code>, <code>ColArchive</code>, <code>GSOD</code> and
              <code>NCARds512</code></li>
          <li>the provenance of specific cell values cannot be asserted; for example, data values
            for 1935 observed at Entebbe are digitised from <a
              href="ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage0/uganda/Uganda_1935-1938.pdf#page=3"
              > digital images published in PDF</a></li>
        </ul>
        <p>A snippet of the data indicating maximum monthly temperature for all stations in Uganda
          from <code>uganda-bestguess</code> is provided below (truncated to 9 columns). File = <a
            type="text/plain" href="ug_tmx_jrc_bg_v1.0.txt">ug_tmx_jrc_bg_v1.0.txt</a></p>

        <pre class="example">
            ARUA	BOMBO	BUKALASA	BUTIABA	DWOLI	ENTEBBE AIR	FT PORTAL	GONDOKORO	[…]
            {snip}
            1935.04	-99.00	-99.00	-99.00	-99.00	-99.00	27.83	-99.00	-99.00	[…]
            1935.12	-99.00	-99.00	-99.00	-99.00	-99.00	25.72	-99.00	-99.00	[…]
            1935.21	-99.00	-99.00	-99.00	-99.00	-99.00	26.44	-99.00	-99.00	[…]
            1935.29	-99.00	-99.00	-99.00	-99.00	-99.00	25.72	-99.00	-99.00	[…]
            1935.37	-99.00	-99.00	-99.00	-99.00	-99.00	24.61	-99.00	-99.00	[…]
            1935.46	-99.00	-99.00	-99.00	-99.00	-99.00	24.33	-99.00	-99.00	[…]
            1935.54	-99.00	-99.00	-99.00	-99.00	-99.00	24.89	-99.00	-99.00	[…]
            {snip}
          </pre>

        <p>Many of the characteristics concerning the “raw” file are exhibited here too.
          Additionally, we see that:</p>
        <ul>
          <li>the delimiter is now tab (<code>U+0009</code>)</li>
          <li>metadata is entirely missing from this file, requiring human intervention to combine
            the filename token (<code>tmx</code>) with supplementary information in the accompanying
            Microsoft Word document to determine the semantics</li>
        </ul>
        <p class="issue">do we support delimiters other than comma, missing (or non-unique) column
          names etc. … or assume that some trivial pre-processor is required to fix up the text?</p>

        <p>At present, the global surface temperature databank comprises 25 Stage 1 data decks for
          monthly temperature observations. These are provided by numerous organisations in
          heterogeneous forms. In order to merge these data decks into a single combined dataset,
          each data deck has to be converted into a standard form. Columns consist of: <code>station
            name</code>, <code>latitude</code>, <code>longitude</code>, <code>altitude</code>,
            <code>date</code>, <code>maximum monthly temperature</code>, <code>minimum monthly
            temperature</code>, <code>mean monthly temperature</code> plus additional provenance
          information.</p>

        <p>An example Stage 2 data file is given for Entebbe, Uganda, below. File = <a
            type="text/plain" href="uganda_000000000005_monthly_stage2"
            >uganda_000000000005_monthly_stage2</a></p>

        <pre class="example">
            {snip}
            ENTEBBE                            0.0500    32.4500  1146.35 193501XX  2783  1711  2247 301/109/101/104/999/999/999/000/000/000/102
            ENTEBBE                            0.0500    32.4500  1146.35 193502XX  2572  1772  2172 301/109/101/104/999/999/999/000/000/000/102
            ENTEBBE                            0.0500    32.4500  1146.35 193503XX  2644  1889  2267 301/109/101/104/999/999/999/000/000/000/102
            ENTEBBE                            0.0500    32.4500  1146.35 193504XX  2572  1817  2194 301/109/101/104/999/999/999/000/000/000/102
            ENTEBBE                            0.0500    32.4500  1146.35 193505XX  2461  1722  2092 301/109/101/104/999/999/999/000/000/000/102
            ENTEBBE                            0.0500    32.4500  1146.35 193506XX  2433  1706  2069 301/109/101/104/999/999/999/000/000/000/102
            ENTEBBE                            0.0500    32.4500  1146.35 193507XX  2489  1628  2058 301/109/101/104/999/999/999/000/000/000/102
            {snip}
          </pre>

        <p>Because of the heterogeneity of the Stage 1 data decks, bespoke data processing programs
          were required for each data deck consuming valuable effort and resource in simple data
          pre-processing. If the semantics, structure and other supplementary metadata pertinent to
          the Stage 1 data decks had been machine readable, then this data homogenisation stage
          could have been avoided altogether. Data provenance is crucial to this initiative,
          therefore it would be beneficial to be able to associate the supplementary metadata
          without needing to edit the original data files.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-LinksToExternallyManagedDefinitions">LinksToExternallyManagedDefinitions</a>,
            <a href="#R-SyntacticTypeDefinition">SyntacticTypeDefinition</a>, <a
            href="#R-SemanticTypeDefinition">SemanticTypeDefinition</a>, <a
            href="#R-MissingValueDefinition">MissingValueDefinition</a> and <a
            href="#R-ZeroEditAdditionOfSupplementaryMetadata"
            >ZeroEditAdditionOfSupplementaryMetadata</a>
        </p>

        <p>The data pre-processing tools created to parse each Stage 1 data deck into the standard
          Stage 2 format and the merge process to create the consolidated Stage 3 data set were
          written using the software most familiar to the participating scientists: <a
            href="http://en.wikipedia.org/wiki/Fortran_95_language_features">Fortran 95</a>. The
          merge software source code is available <a
            href="ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage3/recommended/code/recommended.code.v1.0.0-beta4.20130614.tar.gz"
            >online</a>. It is worth noting that this sector of the scientific community also
          commonly uses <a href="http://www.exelisvis.com/ProductsServices/IDL.aspx">IDL</a> and is
          gradually adopting <a href="http://www.python.org/">Python</a> as the default software
          language choice. </p>

        <p>The resulting merged dataset is published in several formats – including tabular text.
          The <a
            href="ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage3/recommended/results/recommended-ghcn_format.monthly.stage3.v1.0.0-beta4.20130614.tar.gz"
            > GHCN-format merged dataset</a> comprises of several files: merged data and withheld
          data (e.g. those data that did not meet the merge criteria) each with an associated
          “inventory” file.</p>

        <p>A snippet of the inventory for merged data is provided below; each row describing one of
          the 31,427 sites in the dataset. File = <a type="text/plain"
            href="merged.monthly.stage3.v1.0.0-beta4.inv"
          >merged.monthly.stage3.v1.0.0-beta4.inv</a></p>

        <pre class="example">
            {snip}
            REC41011874   0.0500  32.4500 1155.0 ENTEBBE_AIRPO
            {snip}
          </pre>

        <p>The columns are: <code>station identifier</code>, <code>latitude</code>,
            <code>longitude</code>, <code>altitude (m)</code> and <code>station name</code>. The
          data is fixed format rather than delimited.</p>

        <p>Similarly, a snippet of the merged data itself is provided. Given that the original
            <code>.dat</code> file is a largely unmanageable 422.6 MB in size, a subset is provided.
          File = <a type="text/plain" href="merged.monthly.stage3.v1.0.0-beta4.snip"
            >merged.monthly.stage3.v1.0.0-beta4.snip</a></p>

        <pre class="example">
            {snip}
            REC410118741935TAVG 2245    2170    2265    2195    2090    2070    2059    2080    2145    2190    2225    2165   
            REC410118741935TMAX 2780    2570    2640    2570    2460    2430    2490    2520    2620    2630    2660    2590   
            REC410118741935TMIN 1710    1770    1890    1820    1720    1710    1629    1640    1670    1750    1790    1740   
            {snip}
          </pre>

        <p>The columns are: <code>station identifier</code>, <code>year</code>, <code>quantity
            kind</code> and the quantity values for months January to December in that year. Again,
          the data is fixed format rather than delimited.</p>

        <p>Here we see the station identifier <code>REC41011874</code> being used as a foreign key
          to refer to the observing station details; in this case Entebbe Airport. Once again, there
          is no metadata provided within the file to describe how to interpret each of the data
          values.</p>

        <p>
          <strong>Requires:</strong>
          <a href="#R-ForeignKeyReferences">ForeignKeyReferences</a>
        </p>
        
        <p>The resulting merged dataset provides time series of how the observed climate has changed 
          over a long duration at approximately 32000 locations around the globe. Such instrumental
          climate records provide a basis for climate research. However, it is well known that these
          climate records are usually affected by inhomogeneities (artifical shifts) due to changes
          in the measurement conditions (e.g. relocation, modification or recalibration
          of the instrument etc.). As these artificial shifts often have the same magnitude as the
          climate signal, such as long-term variations, trends or cycles, a direct analysis of the
          raw time-series data can lead to wrong conclusions about climate change.</p>
        <p>Statistical homogenisation procedures are used to detect and correct these artificial shifts. 
          Once detected, the raw time-series data is annotated to indicate the presence of artifical
          shifts in the data, details of the homogenisation procedure undertaken and, where possible, 
          the reasons for those shifts.</p>
        
        <p>
          <strong>Requires:</strong>
          <a href="#R-AnnotationAndSupplementaryInfo">AnnotationAndSupplementaryInfo</a>
        </p>
        
        <p>Future iterations of the global land surface temperatures databank are aniticipated to 
          include quality controlled (Stage 4) and homogenised (Stage 5) datasets derived from the 
          merged dataset (Stage 3) outlined above.</p>
        
      </section>
      <section id="UC-ReliabilityAnalysesOfPoliceOpenData">
	<h2>Use Case #8 - Reliability Analyses of Police Open Data</h2>
        <p>
          <span style="font-size: 10pt">(Contributed by Davide Ceolin)</span>
        </p>
	<p>Several Web sources expose datasets about UK crime statistics.
	These datasets vary in format (e.g. maps vs. CSV files), timeliness, aggregation level, etc.
	Before being published on the Web, these data are processed to preserve the privacy of the people
	involved, but again the processing policy varies from source to source.</p>
	<p>Every month, the UK Police Home Office publishes (via data.police.uk) CSV files that report crime
	counts, aggregated on geographical basis (per address or police neighbourhood) and on type basis.
	Before publishing, data are smoothed, that is, grouped in predefined areas and assigned to the
	mid point of each area. Each area has to contain a minimum number of physical addresses. The goal
	of this procedure is to prevent the reconstruction of the identity of the people involved in the
	crimes.</p>
	<p>Over time, the policies adopted for preprocessing these data have changed, but data previously
	published have not been recomputed. Therefore, datasets about different months present relevant
	differences in terms of crime types reported and geographical aggregation (e.g. initially, each
	geographical area for aggregation had to include at least 12 physical addresses. Later, this
	limit was lowered to 8).</p>
	<p>These policies introduce a controlled error in the data for privacy reasons, but these changes
	in the policies imply the fact that different datasets adhere differently to the real data, i.e.
	they present different reliability levels. Previous work [1] provided two procedures for measuring
	and comparing the reliability of the datasets, but in order to automate and improve these procedures,
	it is crucial to understand the meaning of the columns, the relationships between columns, and how the
	data rows have been computed.</p>
	<p>For instance, here is a snippet from a dataset about crime happened in Hampshire in April 2012:
	<pre class="example">
        Month	Force			Neighbourhood	Burglary	Robbery		Vehicle crime	Violent crime	Anti-social behaviour	Other crime
{snip}
2011-04	Hampshire Constabulary	2LE11		2		0		1		6		14			6
2011-04	Hampshire Constabulary	2LE10		1		0		2		4		15			6
2011-04	Hampshire Constabulary	2LE12		3		0		0		4		25			21
{snip}
	</pre>
	and that dataset reports 248 entries, while in October 2012, the crime types we can see are increased to 11:
	<pre class = "example">
	  Month	Force			Neighbourhood Burglary	Robbery	Vehicle crime	Violent crime	ASB	CDA	Shoplifting	Other theft	Drugs	PDW	Other crime
2012-10	Hampshire Constabulary	2LE11		1		0		1		2	8	0		0		1	1	0	1
2012-10	Hampshire Constabulary	1SY01		9		1		12		8	87	17		12		14	13	7	4
2012-10	Hampshire Constabulary	1SY02		11		0		11		20	144	39		2		12	9	8	5
	</pre>
	<ul>
	  <li>ASB = Anti-social behaviour</li>
	  <li>PDW = Public Disorder and Weapons</li>
	  <li>CDA = Criminal damage and arson</li>
	</ul>
	This dataset reports 232 entries.</p>
	
	<p>In order to properly handle the columns,
	it is crucial to understand the type of the data therein contained. Given the context, knowing
	this information would reveal an important part of the column meaning (e.g. to identify dates).
	</p>
	<p>
          <strong>Requires:</strong>
          <a href="#R-SyntacticTypeDefinition">SyntacticTypeDefinition</a>
        </p>
	<p>
	Also, it is important to understand the precise semantics of each column.
	This is relevant for two reasons. First, to identify relations between columns (e.g. some crime types
	are siblings, while other are less semantically related). Second, to identify semantic relations between
	columns in heterogeneous datasets (e.g. a column in one dataset may correspond to the sum of two or more
	columns in others).
	</p>
	<p>
          <strong>Requires:</strong>
          <a href="#R-SemanticTypeDefinition">SemanticTypeDefinition</a>
        </p>
	<p>Lastly, datasets with different row numbers are the result of different smoothing procedures. Therefore, it would
	be important to trace and access their provenance, in order to facilitate their comparison.</p>
	<p>
          <strong>Requires:</strong>
          <a href="#R-ProvenanceRecording">ProvenanceRecording</a>
        </p>
      </section>

      <section id="other-uc">
        <h2>Other Use Cases</h2>
        <p> Don't know if we need this yet? </p>
      </section>

    </section>
    <section id="req">
      <h2>Requiremnents</h2>
      <section id="acc-req">
        <h2>Accepted Requirements</h2>
        <p>...</p>
        <div class="issue">
          <p>Proposal to cluster requirements - candidate clusters are:</p>
          <ul>
            <li>Parsing, eg requirements around recognising other delimiters</li>
            <li>Annotation Types, eg R-PrimaryKey</li>
            <li>Metadata Discovery, eg R-PackagingOfMultipleTables</li>
            <li>Applications, eg R-CsvValidation</li>
            <li>Non-Functional, eg R-ZeroEditCompatibility</li>
          </ul>
        </div>
      </section>
      <section id="can-req">
        <h2>Candidate Requirements</h2>
        <dl>
          <dt id="R-PrimaryKey">R-PrimaryKey</dt>
          <dd>
            <em id="_R-PrimaryKey">
              <strong>Ability to determine the primary key for entities described within a CSV
                file</strong>
            </em>
            <p>Each row within a CSV file typically relates to a single entity. In many cases that
              entity is the object of references from other entities described within the CSV file -
              or perhaps even from entities described in other CSV files or data resources.</p>
            <p>Typically within a CSV file, primary key identifiers are only unique within the scope
              of the CSV file within which they are stated (e.g. a local identifier). In order for
              the entity to be unambiguously identified, the local identifier needs to be converted
              to a URI (as defined in [[!RFC3986]]).</p>
            <ol>
              <li>need to determine which cell provides the primary key in a given row.</li>
              <li>where the primary key is defined as a local identfier, need to determine how to
                convert the local identifier to a URI.</li>
            </ol>
            <p class="note">Assumption that a row within a CSV file describes a <em>single</em>
              entity for which a primary key can be assigned.</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>
            </p>
          </dd>
          <dt id="R-ForeignKeyReferences">R-ForeignKeyReferences</dt>
          <dd>
            <em id="_R-ForeignKeyReferences">
              <strong>Ability to cross reference between CSV files</strong>
            </em>
            <p>To interpret data in a given row of a CSV file, need to be able to refer to
              information provided in supplementary CSV files or elsewhere within the same CSV file;
              e.g. using a foreign key type reference. The cross-referenced CSV files may, or may
              not, be packaged together.</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>, <a
                href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
          <dt id="R-ExternalDataDefinitionResource">R-ExternalDataDefinitionResource</dt>
          <dd>
            <em id="_R-ExternalDataDefinitionResource">
              <strong>Ability to reference a Data Definition Resource defining supplementary
                metadata external to the CSV file</strong>
            </em>
            <p>To allow automated processing of a CSV file additional metadata is required to
              describe the structure and semantics of that file. This additional metadata is termed
              a <em>Data Definition Resource</em> (<dfn>DDR</dfn>). The <a>DDR</a> may be defined
              outside the scope of the CSV file with which it is associated; for example, if the
                <a>DDR</a> is common to many CSV files or the <a>DDR</a> is used to drive CSV file
              validation. In such cases it must be possible to associate the <a>DDR</a> from the CSV
              file.</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>
            </p>
          </dd>
          <dt id="R-CsvValidation">R-CsvValidation</dt>
          <dd>
            <em id="_R-CsvValidation">
              <strong>Ability to validate a CSV for conformance with a specified <a>DDR</a></strong>
            </em>
            <p>The content of a CSV often needs to be validated for conformance against a
              specification (e.g. a <a>DDR</a>). The validation applied may vary based on
              row-specific attributes such as the type of entity described in that row.</p>
            <p class="issue">Is CSV validation in scope?</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>
            </p>
          </dd>
          <dt id="R-CsvToRdfTransformation">R-CsvToRdfTransformation</dt>
          <dd>
            <em id="_R-CsvToRdfTransformation">
              <strong>Ability to automatically transform a CSV into RDF</strong>
            </em>
            <p>Standardised CSV to RDF transformation mechanisms mitigate the need for bespoke
              transformation software to be developed by CSV data consumers, thus simplifying the
              exploitation of CSV data. Identifiers used as primary and foreign keys within a CSV
              file need to be converted to URIs. RDF properties (or <em>property paths</em>) need to
              be determined to relate the entity described within a given row to the corresponding
              data values for that row. Where available, the type of a data value should be
              incorporated in the resulting RDF. Built-in types defined in [[RDF-CONCEPTS]] (e.g.
                  <code><a href="http://www.w3.org/2001/XMLSchema#dateTime">xsd:dateTime</a></code>,
                  <code><a href="http://www.w3.org/2001/XMLSchema#integer">xsd:integer</a></code>
              etc.) and types defined in other RDF vocabularies / OWL ontologies (e.g. <code><a
                  href="http://www.opengis.net/ont/geosparql#wktLiteral">geo:wktLiteral</a></code>
              from <a href="http://www.opengeospatial.org/standards/geosparql">GeoSPARQL</a>) shall
              be supported.</p>
            <p class="issue">Should we be referencing RDF 1.1?</p>
            <p>
              <strong>Dependency:</strong>
              <a href="#R-SemanticTypeDefinition">R-SemanticTypeDefinition</a>, <a
                href="#R-SyntacticTypeDefinition">R-SyntacticTypeDefinition</a>, <a
                href="#R-PrimaryKey">R-PrimaryKey</a>
            </p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>
            </p>
          </dd>

          <dt id="R-PackagingOfMultipleTables">R-PackagingOfMultipleTables</dt>
          <dd>
            <em id="_R-PackagingOfMultipleTables">
              <strong>Ability to group multiple data tables into a single package for
                publication</strong>
            </em>
            <p>When publishing sets of related data tables, it shall be possible to group them
              together into a single package for publication. The package may optionally include a
              manifest of the tables contained therein and summary information pertinent to the
              whole package.</p>
            <p>Furthermore, where appropriate, it shall be possible to describe the interrelationships
              between the tabular datasets within the published group.</p>
            <div class="issue">
              <p>"Regarding the requirement R-PackagingOfMultipleTables, I think the requirement is
                to annotate a group of tables, not necessarily to package them. In other words, a
                design in which there was a metadata file that pointed to a group of tables hosted
                elsewhere on the web would seem to satisfy the requirement from
                PublicationOfNationalStatistics: they wouldn’t necessarily need to be packaged
                together (eg in a zip)."</p>
              <p>
                <span style="font-size: 10pt">[Jeni Tennison]</span>
              </p>
            </div>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-PublicationOfNationalStatistics">PublicationOfNationalStatistics</a>
            </p>
          </dd>
          <dt id="R-AnnotationAndSupplementaryInfo">R-AnnotationAndSupplementaryInfo</dt>
          <dd>
            <em id="_R-AnnotationAndSupplementaryInfo">
              <strong>Ability to add annotation and supplementary information to CSV file</strong>
            </em>
            <p>Annotations and supplementary information may be associated with:</p>
            <ul>
              <li>a group of tables</li>
              <li>an individual table</li>
              <li>a range (or region) of cells within a table</li>
              <li>a row</li>
              <li>a coloumn</li>
              <li>an individual cell</li>
            </ul>
            <p>Annotations and supplementary information may be literal values or references to a
              remote resource. The presence of annotations or supplementary information must not
              adversely impact parsing of the tabular data (e.g. the annotations and supplementary
              information must be logically separate).</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-PublicationOfNationalStatistics">PublicationOfNationalStatistics</a>, <a
                href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
          <dt id="R-AssociationOfCodeValuesWithExternalDefinitions"
            >R-AssociationOfCodeValuesWithExternalDefinitions</dt>
          <dd>
            <em id="_R-AssociationOfCodeValuesWithExternalDefinitions">
              <strong>Ability to associate a code value with externally managed defintion</strong>
            </em>
            <p>CSV files make frequent use of code values when describing data. Examples include:
              geographic regions, status codes and category codes. It is difficult to interpret the
              tabular data with out an unambiguous definition of the code values used.</p>
            <p>It must be possible to unambiguously associate the notation used within a CSV file
              with the appropriate external definition.</p>
            <div class="note">
              <p>We cannot assume that the publisher of the CSV file will use a URI to reference the
                code value; most likely they will use a local identifier that is unique within the
                scope of a particular code list. For example, the Land Registry use the codes "A",
                "C" and "D" to denote their transactions rather than a fully qualified URI reference
                to the concept that these codes identify.</p>
              <p>Thus the requirement here is two fold:</p>
              <ul>
                <li>to refer to the code list (aka thesaurus) wherein the code value is defined,
                  and</li>
                <li>to determine the relevant entity within the code list using the notation
                  provided in the cell value.</li>
              </ul>
            </div>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-PublicationOfNationalStatistics">PublicationOfNationalStatistics</a>, <a
                href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
          <dt id="R-ZeroEditCompatibility">R-ZeroEditCompatibility</dt>
          <dd>
            <em id="_R-ZeroEditCompatibility">
              <strong>Compatibility of data analysis tools in common usage with CSV+</strong>
            </em>
            <p>CSV+ format should be compatible, at least at a basic level, with the data analysis
              tools in common usage. At a minimum, existing tools should be able to interpret CSV+
              as though it were CSV (as defined in [[RFC4180]]).</p>
            <div class="issue">
              <p>Whilst we have a list of CSV tools emerging on the <a
                  href="https://www.w3.org/2013/csvw/wiki/Tools">wiki</a> we are yet to define the
                list of tools with which we expect, or aspire, to retain compatibility with.</p>
              <p>We also do not have an authoritative set of conformance tests yet.</p>
            </div>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-PublicationOfNationalStatistics">PublicationOfNationalStatistics</a>
            </p>
          </dd>
          <dt id="R-CsvAsSubsetOfLargerDataset">R-CsvAsSubsetOfLargerDataset</dt>
          <dd>
            <em id="_R-CsvAsSubsetOfLargerDataset">
              <strong>Ability to assert how a single CSV file is a facet or subset of a larger
                dataset</strong>
            </em>
            <p>description to be added here</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
          <dt id="R-LinksToExternallyManagedDefinitions">R-LinksToExternallyManagedDefinitions</dt>
          <dd>
            <em id="_R-LinksToExternallyManagedDefinitions">
              <strong>Ability to provide (hyper)links to externally managed definitions from with a
                CSV file</strong>
            </em>
            <p>description to be added here</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
          <dt id="R-SyntacticTypeDefinition">R-SyntacticTypeDefinition</dt>
          <dd>
            <em id="_R-SyntacticTypeDefinition">
              <strong>Ability to declare syntactic type for data values</strong>
            </em>
            <p>supporting automated recognition syntactic type e.g. date, number etc. ... further
              description to be added</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>, <a
                href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>
		, <a href="#UC-ReliabilityAnalysesOfPoliceOpenData">ReliabilityAnalysesOfPoliceOpenData</a>
            </p>
          </dd>
          <dt id="R-SemanticTypeDefinition">R-SemanticTypeDefinition</dt>
          <dd>
            <em id="_R-SemanticTypeDefinition">
              <strong>Ability to declare semantic type for data values</strong>
            </em>
            <p>supporting automated recognition of semantic type, typically expressed for each
              column ... further description to be added</p>
            <div class="note">
              <p>To express semantics in a machine readable form, RDF seems the appropriate choice.
                Furthermore, best practice indicates that one should adopt common and widely adopted
                patterns (e.g. RDF vocabularies, OWL ontologies) when publishing data to enable a
                wide audience to consume and understand the data. Existing (de facto) standard
                patterns may add complexity when defining the semantics associated with a particular
                row such that a single RDF predicate is insufficient. </p>
              <p>For example, to express a quantity value using <a href="http://qudt.org/">QUDT</a>
                we use an instance of <code>qudt:QuantityValue</code> to relate the numerical value
                with the quantity kind (e.g. air temperature) and unit of measurement (e.g.
                Celsius). Thus the semantics needed for a column containing temperature values might
                be: <code>qudt:value/qudt:numericValue</code> – more akin to a <a
                  href="http://marmotta.apache.org/ldpath/language.html">LDPath</a>.</p>
              <p>Furthermore, use of OWL axioms when defining a sub-property of
                  <code>qudt:value</code> would allow the quantity type and unit of measurement to
                be inferred, with the column semantics then being specified as
                  <code>ex:temperature_Cel/qudt:numericValue</code>.</p>
            </div>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-DigitalPreservationOfGovernmentRecords"
                >DigitalPreservationOfGovernmentRecords</a>, <a
                href="#UC-PublicationOfNationalStatistics">PublicationOfNationalStatistics</a>,<a
                href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
		, <a href="#UC-ReliabilityAnalysesOfPoliceOpenData">ReliabilityAnalysesOfPoliceOpenData</a>
            </p>
          </dd>
          <dt id="R-MissingValueDefinition">R-MissingValueDefinition</dt>
          <dd>
            <em id="_R-MissingValueDefinition">
              <strong>Ability to declare a "missing value" token and, optionally, a reason for the
                value to be missing</strong>
            </em>
            <p>Significant amounts of existing tabular text data include values such as
                <code>-999</code>. Typically, these are outside the normal expected range of values
              and are meant to infer that the value for that cell is missing. Automated parsing of
              CSV files needs to recognise such missing value tokens and behave accordingly.
              Furthermore, it is often useful for a data publisher to declare <em>why</em> a value
              is missing; e.g. <code>withheld</code> or <code>aboveMeasurementRange</code></p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
          <dt id="R-ZeroEditAdditionOfSupplementaryMetadata"
            >R-ZeroEditAdditionOfSupplementaryMetadata</dt>
          <dd>
            <em id="_R-ZeroEditAdditionOfSupplementaryMetadata">
              <strong>Ability to add supplementary metadata to an existing CSV file without
                requiring modification of that file</strong>
            </em>
            <p>description to be added here</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-SurfaceTemperatureDatabank">SurfaceTemperatureDatabank</a>
            </p>
          </dd>
	  <dt id="R-ProvenanceRecording">R-ProvenanceRecording</dt>
          <dd>
            <em id="_R-ProvenanceRecording">
              <strong>Ability to keep track of the provenance trace of a CSV file</strong>
            </em>
            <p>The ability to capture the provenance of a CSV file or of part of it is crucial to understand and properly manage the data therein contained.</p>
	    <p class="issue">Should we distinguish between the provenance of part of the CSV file (e.g. of some cells) and the provenance of the file as a whole?</p>
            <p>
              <strong>Motivation:</strong>
              <a href="#UC-ReliabilityAnalysesOfPoliceOpenData">ReliabilityAnalysesOfPoliceOpenData</a>
            </p>
          </dd>
        </dl>
      </section>
    </section>
    <section id="conc">
      <h2>Conclusions</h2>
      <p> ... </p>
    </section>
  </body>
</html>
